{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/destroyer000lucky/Gymnasium/blob/main/ReplayBufferPruningCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_USkJU4HKcuE"
      },
      "outputs": [],
      "source": [
        "# define a replay buffer\n",
        "import collections\n",
        "import typing\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "_field_names = [\n",
        "    \"state\",\n",
        "    \"action\",\n",
        "    \"reward\",\n",
        "    \"next_state\",\n",
        "    \"done\"\n",
        "]\n",
        "Experience = collections.namedtuple(\"Experience\", field_names=_field_names)\n",
        "\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 batch_size: int,\n",
        "                 buffer_size: int = None,\n",
        "                 random_state: np.random.RandomState = None) -> None:\n",
        "        \"\"\"\n",
        "        Initialize an ExperienceReplayBuffer object.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        buffer_size (int): maximum size of buffer\n",
        "        batch_size (int): size of each training batch\n",
        "        seed (int): random seed\n",
        "        \n",
        "        \"\"\"\n",
        "        self._batch_size = batch_size\n",
        "        self._buffer_size = buffer_size\n",
        "        self._buffer = collections.deque(maxlen=buffer_size)\n",
        "        self._random_state = np.random.RandomState() if random_state is None else random_state\n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        return len(self._buffer)\n",
        "    \n",
        "    @property\n",
        "    def batch_size(self) -> int:\n",
        "        return self._batch_size\n",
        "    \n",
        "    @property\n",
        "    def buffer_size(self) -> int:\n",
        "        return self._buffer_size\n",
        "\n",
        "    def is_full(self) -> bool:\n",
        "        return len(self._buffer) == self._buffer_size\n",
        "    \n",
        "    def append(self, experience: Experience) -> None:\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        self._buffer.append(experience)\n",
        "    \n",
        "    def sample(self, deleted) -> typing.List[Experience]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = []\n",
        "        idxs = self._random_state.randint(len(self._buffer), size=self._batch_size)\n",
        "        for i in range(len(idxs)): \n",
        "          if (idxs[i] not in deleted):\n",
        "            experiences.append(self._buffer[idxs[i]])\n",
        "        # experiences = [self._buffer[idx] for idx in idxs]\n",
        "        return experiences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJZvaM3bLHaD",
        "outputId": "d283263c-b765-4ba9-db64-82509d5e7833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[box2d]==0.17.*\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.8/dist-packages (3.1.6)\n",
            "Collecting PyOpenGL-accelerate==3.1.*\n",
            "  Downloading PyOpenGL-accelerate-3.1.6.tar.gz (550 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m550.6/550.6 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from gym[box2d]==0.17.*) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]==0.17.*) (1.21.6)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting box2d-py~=2.3.5\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n",
            "Building wheels for collected packages: PyOpenGL-accelerate, box2d-py, gym\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.6-cp38-cp38-linux_x86_64.whl size=2508167 sha256=95f53115696a0716765fd64e8263e816da6d919e8d48b6854fb679275466c378\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/04/74/854c06008a26af5a3b8106007e2950b70df3349324239586f2\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654651 sha256=2b051916847ba1fdf5500969f05d38e07aa04cc294042dd15b132648c9e49a23\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/40/e7/14efb9870cfc92ac236d78cb721dce614ddec9666c8a5e0a35\n",
            "Successfully built PyOpenGL-accelerate gym\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: PyOpenGL-accelerate, EasyProcess, box2d-py, pyvirtualdisplay, pyglet, cloudpickle, gym\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for box2d-py\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
            "\u001b[31m╰─>\u001b[0m box2d-py\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ribs[visualize]\n",
            "  Downloading ribs-0.4.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym~=0.17.0\n",
            "  Using cached gym-0.17.3-py3-none-any.whl\n",
            "Collecting Box2D~=2.3.10\n",
            "  Downloading Box2D-2.3.10-cp38-cp38-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: ribs 0.4.0 does not provide the extra 'visualize'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: toml>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from ribs[visualize]) (0.10.2)\n",
            "Requirement already satisfied: decorator>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from ribs[visualize]) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from ribs[visualize]) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ribs[visualize]) (3.1.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ribs[visualize]) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from ribs[visualize]) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from ribs[visualize]) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ribs[visualize]) (1.3.5)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.8/dist-packages (from ribs[visualize]) (0.56.4)\n",
            "Collecting cloudpickle<1.7.0,>=1.2.0\n",
            "  Using cached cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0\n",
            "  Using cached pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.51.0->ribs[visualize]) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.51.0->ribs[visualize]) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.51.0->ribs[visualize]) (6.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.0->ribs[visualize]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.0->ribs[visualize]) (2022.7.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.0) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->ribs[visualize]) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->ribs[visualize]) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.51.0->ribs[visualize]) (3.12.0)\n",
            "Installing collected packages: Box2D, pyglet, cloudpickle, gym, ribs\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.2.1\n",
            "    Uninstalling cloudpickle-2.2.1:\n",
            "      Successfully uninstalled cloudpickle-2.2.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed Box2D-2.3.10 cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0 ribs-0.4.0\n"
          ]
        }
      ],
      "source": [
        "#%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "#apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "%pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "%pip install ribs[visualize] gym~=0.17.0 Box2D~=2.3.10\n",
        "#%pip install pyvirtualdisplay==0.2.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVEPDxmHLfAq"
      },
      "outputs": [],
      "source": [
        "#import pyvirtualdisplay\n",
        "\n",
        "\n",
        "#_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    #size=(1400, 900))\n",
        "#_ = _display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1YbQNW2LjYi"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "_ = env.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx81pASoLrGK"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    \n",
        "    def choose_action(self, state: np.array) -> int:\n",
        "        \"\"\"Rule for choosing an action given the current state of the environment.\"\"\"\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def save(self, filepath) -> None:\n",
        "        \"\"\"Save any important agent state to a file.\"\"\"\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def step(self,\n",
        "             state: np.array,\n",
        "             action: int,\n",
        "             reward: float,\n",
        "             next_state: np.array,\n",
        "             done: bool) -> None:\n",
        "        \"\"\"Update agent's state after observing the effect of its action on the environment.\"\"\"\n",
        "        raise NotImplmentedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Iba3-SALvij"
      },
      "outputs": [],
      "source": [
        "def _train_for_at_most(agent: Agent, env: gym.Env, max_timesteps: int) -> int:\n",
        "    \"\"\"Train agent for a maximum number of timesteps.\"\"\"\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    for t in range(max_timesteps):\n",
        "        action = agent.choose_action(state)   # epislon action generation policy\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.step(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        score += reward\n",
        "        if done:\n",
        "            break\n",
        "    return score\n",
        "\n",
        "                \n",
        "def _train_until_done(agent: Agent, env: gym.Env) -> float:\n",
        "    \"\"\"Train the agent until the current episode is complete.\"\"\"\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.step(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        score += reward\n",
        "    return score\n",
        "\n",
        "import time\n",
        "\n",
        "def train(agent: Agent,\n",
        "          env: gym.Env,\n",
        "          checkpoint_filepath: str,\n",
        "          target_score: float,\n",
        "          number_episodes: int,\n",
        "          maximum_timesteps=None) -> typing.List[float]:\n",
        "    \"\"\"\n",
        "    Reinforcement learning training loop.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    agent (Agent): an agent to train.\n",
        "    env (gym.Env): an environment in which to train the agent.\n",
        "    checkpoint_filepath (str): filepath used to save the state of the trained agent.\n",
        "    number_episodes (int): maximum number of training episodes.\n",
        "    maximum_timsteps (int): maximum number of timesteps per episode.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    scores (list): collection of episode scores from training.\n",
        "    \n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    most_recent_scores = collections.deque(maxlen=25)\n",
        "    for i in range(number_episodes):\n",
        "        #start = time.time()\n",
        "        if maximum_timesteps is None:\n",
        "            score = _train_until_done(agent, env)\n",
        "        else:\n",
        "            score = _train_for_at_most(agent, env, maximum_timesteps)         \n",
        "        scores.append(score)\n",
        "        most_recent_scores.append(score)\n",
        "        \n",
        "        average_score = sum(most_recent_scores) / len(most_recent_scores)\n",
        "        if average_score >= target_score:\n",
        "            print(f\"\\nEnvironment solved in {i:d} episodes!\\tAverage Score: {average_score:.2f}\")\n",
        "            agent.save(checkpoint_filepath)\n",
        "            break\n",
        "        if (i + 1) % 25 == 0:\n",
        "            print(f\"\\rEpisode {i + 1}\\tAverage Score: {average_score:.2f}\")\n",
        "        #end = time.time()\n",
        "        #print(end-start)\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF1Ir3GzL62h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "import random \n",
        "import numpy as np\n",
        "import time\n",
        "import bisect\n",
        "\n",
        "from torch import nn\n",
        "class Net(nn.Module):\n",
        "  \"\"\"A non-sparse neural network with four hidden fully-connected layers\"\"\"\n",
        "\n",
        "  def __init__(self,_state_size, number_hidden_units, _action_size):\n",
        "    super(Net,self).__init__()\n",
        "    self.input_layer = nn.Linear(_state_size, number_hidden_units, bias=False)\n",
        "    self.hidden1_layer = nn.Linear(number_hidden_units, number_hidden_units, bias=False)\n",
        "    self.output_layer = nn.Linear(number_hidden_units, _action_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.input_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden1_layer(x)\n",
        "    x = F.relu(x)\n",
        "    output = self.output_layer(x)\n",
        "\n",
        "    return output\n",
        "\n",
        "class DeepQAgent(Agent):\n",
        "\n",
        "    def __init__(self,\n",
        "                 state_size: int,\n",
        "                 action_size: int,\n",
        "                 number_hidden_units: int,\n",
        "                 optimizer_fn: typing.Callable[[typing.Iterable[torch.nn.Parameter]], optim.Optimizer],\n",
        "                 batch_size: int,\n",
        "                 buffer_size: int,\n",
        "                 epsilon_decay_schedule: typing.Callable[[int], float],\n",
        "                 alpha: float,\n",
        "                 gamma: float,\n",
        "                 update_frequency: int,\n",
        "                 seed: int = None) -> None:\n",
        "        \"\"\"\n",
        "        Initialize a DeepQAgent.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        state_size (int): the size of the state space.\n",
        "        action_size (int): the size of the action space.\n",
        "        number_hidden_units (int): number of units in the hidden layers.\n",
        "        optimizer_fn (callable): function that takes Q-network parameters and returns an optimizer.\n",
        "        batch_size (int): number of experience tuples in each mini-batch.\n",
        "        buffer_size (int): maximum number of experience tuples stored in the replay buffer.\n",
        "        epsilon_decay_schdule (callable): function that takes episode number and returns epsilon.\n",
        "        alpha (float): rate at which the target q-network parameters are updated.\n",
        "        gamma (float): Controls how much that agent discounts future rewards (0 < gamma <= 1).\n",
        "        update_frequency (int): frequency (measured in time steps) with which q-network parameters are updated.\n",
        "        seed (int): random seed\n",
        "        \n",
        "        \"\"\"\n",
        "        self._state_size = state_size\n",
        "        self._action_size = action_size\n",
        "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        # set seeds for reproducibility\n",
        "        self._random_state = np.random.RandomState() if seed is None else np.random.RandomState(seed)\n",
        "        if seed is not None:\n",
        "            torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "        \n",
        "        # initialize agent hyperparameters\n",
        "        self._experience_replay_buffer = ExperienceReplayBuffer(batch_size, buffer_size, seed)\n",
        "        self._epsilon_decay_schedule = epsilon_decay_schedule\n",
        "        self._alpha = alpha\n",
        "        self._gamma = gamma\n",
        "        \n",
        "        # initialize Q-Networks\n",
        "        self._update_frequency = update_frequency\n",
        "        self._local_q_network = self._initialize_q_network(number_hidden_units)\n",
        "        self._target_q_network = self._initialize_q_network(number_hidden_units)\n",
        "        self._synchronize_q_networks()\n",
        "        \n",
        "        # send the networks to the device\n",
        "        self._local_q_network.to(self._device)\n",
        "        self._target_q_network.to(self._device)\n",
        "        \n",
        "        # initialize the optimizer\n",
        "        self._optimizer = optimizer_fn(self._local_q_network.parameters())\n",
        "\n",
        "        # initialize some counters\n",
        "        self._number_episodes = 0\n",
        "        self._number_timesteps = 0\n",
        "        self._number_parameter_updates = 0\n",
        "        \n",
        "    def _initialize_q_network(self, number_hidden_units: int) -> nn.Module:\n",
        "        \"\"\"Create a neural network for approximating the action-value function.\"\"\"\n",
        "        '''\n",
        "        q_network = nn.Sequential(\n",
        "            nn.Linear(in_features=self._state_size, out_features=number_hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=number_hidden_units, out_features=number_hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=number_hidden_units, out_features=self._action_size)\n",
        "        )\n",
        "        '''\n",
        "        q_network = Net(self._state_size, number_hidden_units, self._action_size)\n",
        "        return q_network\n",
        "        \n",
        "    def _learn_from(self, experiences: typing.List[Experience]) -> None:\n",
        "        \"\"\"Heart of the Deep Q-learning algorithm.\"\"\"\n",
        "        states, actions, rewards, next_states, dones = (torch.Tensor(np.array(vs)).to(self._device) for vs in zip(*experiences))\n",
        "        \n",
        "        # get max predicted Q values (for next states) from target model\n",
        "        next_target_q_values, _ = (self._target_q_network(next_states)\n",
        "                                       .detach()\n",
        "                                       .max(dim=1))\n",
        "        \n",
        "        # compute the new Q' values using the Q-learning formula\n",
        "        target_q_values = rewards + (self._gamma * next_target_q_values * (1 - dones))\n",
        "        \n",
        "        # get expected Q values from local model\n",
        "        _index = (actions.long()\n",
        "                         .unsqueeze(dim=1))\n",
        "        expected_q_values = (self._local_q_network(states)\n",
        "                                 .gather(dim=1, index=_index))\n",
        "        # compute the mean squared loss\n",
        "        loss = F.mse_loss(expected_q_values, target_q_values.unsqueeze(dim=1))\n",
        "        \n",
        "        # agent updates the parameters theta of Q using gradient descent\n",
        "        self._optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self._optimizer.step()\n",
        "        \n",
        "        self._soft_update_target_q_network_parameters()\n",
        "                 \n",
        "    def _soft_update_target_q_network_parameters(self) -> None:\n",
        "        \"\"\"Soft-update of target q-network parameters with the local q-network parameters.\"\"\"\n",
        "        for target_param, local_param in zip(self._target_q_network.parameters(), self._local_q_network.parameters()):\n",
        "            target_param.data.copy_(self._alpha * local_param.data + (1 - self._alpha) * target_param.data)\n",
        "    \n",
        "    def _synchronize_q_networks(self) -> None:\n",
        "        \"\"\"Synchronize the target_q_network and the local_q_network.\"\"\"\n",
        "        _ = self._target_q_network.load_state_dict(self._local_q_network.state_dict())\n",
        "           \n",
        "    def _uniform_random_policy(self, state: torch.Tensor) -> int:\n",
        "        \"\"\"Choose an action uniformly at random.\"\"\"\n",
        "        return self._random_state.randint(self._action_size)\n",
        "        \n",
        "    def _greedy_policy(self, state: torch.Tensor) -> int:\n",
        "        \"\"\"Choose an action that maximizes the action_values given the current state.\"\"\"\n",
        "        # evaluate the network to compute the action values\n",
        "        self._local_q_network.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self._local_q_network(state)\n",
        "        \n",
        "        self._local_q_network.train()\n",
        "        \n",
        "        # choose the greedy action\n",
        "        action = (action_values.cpu()  # action_values might reside on the GPU!\n",
        "                               .argmax()\n",
        "                               .item())\n",
        "        return action\n",
        "    \n",
        "    def _epsilon_greedy_policy(self, state: torch.Tensor, epsilon: float) -> int:\n",
        "        \"\"\"With probability epsilon explore randomly; otherwise exploit knowledge optimally.\"\"\"\n",
        "        if self._random_state.random() < epsilon:\n",
        "            action = self._uniform_random_policy(state)\n",
        "        else:\n",
        "            action = self._greedy_policy(state)\n",
        "        return action\n",
        "\n",
        "    def choose_action(self, state: np.array) -> int:\n",
        "        \"\"\"\n",
        "        Return the action for given state as per current policy.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        state (np.array): current state of the environment.\n",
        "        \n",
        "        Return:\n",
        "        --------\n",
        "        action (int): an integer representing the chosen action.\n",
        "\n",
        "        \"\"\"\n",
        "        # need to reshape state array and convert to tensor\n",
        "        state_tensor = (torch.from_numpy(state)\n",
        "                             .unsqueeze(dim=0)\n",
        "                             .to(self._device))\n",
        "        # choose uniform at random if agent has insufficient experience\n",
        "        if not self.has_sufficient_experience():\n",
        "            action = self._uniform_random_policy(state_tensor)\n",
        "        else:\n",
        "            epsilon = self._epsilon_decay_schedule(self._number_episodes)\n",
        "            action = self._epsilon_greedy_policy(state_tensor, epsilon)\n",
        "        return action\n",
        "    \n",
        "    def has_sufficient_experience(self) -> bool:\n",
        "        \"\"\"True if agent has enough experience to train on a batch of samples; False otherwise.\"\"\"\n",
        "        return len(self._experience_replay_buffer) >= self._experience_replay_buffer.batch_size\n",
        "    \n",
        "    def save(self, filepath: str) -> None:\n",
        "        \"\"\"\n",
        "        Saves the state of the DeepQAgent.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        filepath (str): filepath where the serialized state should be saved.\n",
        "        \n",
        "        Notes:\n",
        "        ------\n",
        "        The method uses `torch.save` to serialize the state of the q-network, \n",
        "        the optimizer, as well as the dictionary of agent hyperparameters.\n",
        "        \n",
        "        \"\"\"\n",
        "        checkpoint = {\n",
        "            \"q-network-state\": self._local_q_network.state_dict(),\n",
        "            \"optimizer-state\": self._optimizer.state_dict(),\n",
        "            \"agent-hyperparameters\": {\n",
        "                \"alpha\": self._alpha,\n",
        "                \"batch_size\": self._experience_replay_buffer.batch_size,\n",
        "                \"buffer_size\": self._experience_replay_buffer.buffer_size,\n",
        "                \"gamma\": self._gamma,\n",
        "                \"update_frequency\": self._update_frequency\n",
        "            }\n",
        "        }\n",
        "        torch.save(checkpoint, filepath)\n",
        "    \n",
        "    def step(self, state: np.array, action: int, reward: float, next_state: np.array, done: bool) -> None:\n",
        "        \"\"\"\n",
        "        Updates the agent's state based on feedback received from the environment.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        state (np.array): the previous state of the environment.\n",
        "        action (int): the action taken by the agent in the previous state.\n",
        "        reward (float): the reward received from the environment.\n",
        "        next_state (np.array): the resulting state of the environment following the action.\n",
        "        done (bool): True is the training episode is finised; false otherwise.\n",
        "        \n",
        "        \"\"\"\n",
        "        # save experience in the experience replay buffer\n",
        "        experience = Experience(state, action, reward, next_state, done)\n",
        "        self._experience_replay_buffer.append(experience)\n",
        "            \n",
        "        if done:\n",
        "            self._number_episodes += 1\n",
        "        else:\n",
        "            self._number_timesteps += 1\n",
        "\n",
        "            # every so often the agent should learn from experiences\n",
        "            if self._number_timesteps % self._update_frequency == 0 and self.has_sufficient_experience():\n",
        "                deleted = self.parameter_prune(1.6) # ATTENTION: ADJUST ALGORITHM & VARIABLES HERE!!!\n",
        "                #print(\"Deleted: \" + str(len(deleted)))\n",
        "                #print(\"Replay Buffer: \" + str(len(self._experience_replay_buffer)))\n",
        "                #print(\"Ratio: \" + str(float(len(deleted))/float(len(self._experience_replay_buffer))))\n",
        "\n",
        "                '''\n",
        "                Parameter to Ratios:\n",
        "                0.1 -> 3%\n",
        "                1 -> 35%\n",
        "                1.6 -> 52%\n",
        "                2.5 -> 70%\n",
        "                '''\n",
        "                experiences = self._experience_replay_buffer.sample(deleted)\n",
        "                self._learn_from(experiences)\n",
        "\n",
        "    # def interval_prune(self, parameter, percentage):\n",
        "    #   '''start = time.time()\n",
        "    #   print(\"Interval pruning at: \" + str(start))'''\n",
        "    #   average = [0,0,0,0,0,0]\n",
        "    #   deleted = []\n",
        "    #   i = 0\n",
        "    #   while i < min(500, percentage*len(self._experience_replay_buffer)):\n",
        "    #     etr = self._experience_replay_buffer._buffer[i][0]\n",
        "    #     average[0] += etr[0]/len(self._experience_replay_buffer)\n",
        "    #     average[1] += etr[1]/len(self._experience_replay_buffer)\n",
        "    #     average[2] += etr[2]/len(self._experience_replay_buffer)\n",
        "    #     average[3] += etr[3]/len(self._experience_replay_buffer)\n",
        "    #     average[4] += etr[4]/len(self._experience_replay_buffer)\n",
        "    #     average[5] += etr[5]/len(self._experience_replay_buffer)\n",
        "    #     i += 1\n",
        "    #   i = 0\n",
        "    #   '''firstCompletion = time.time()\n",
        "    #   print(\"First loop complete at: \" + str(firstCompletion))\n",
        "    #   print(str(firstCompletion - start))\n",
        "    #   print(min(500, percentage*len(self._experience_replay_buffer)))'''\n",
        "    #   while i < len(self._experience_replay_buffer):\n",
        "    #     etr = self._experience_replay_buffer._buffer[i][0]\n",
        "    #     distance = ((average[0]-etr[0])**2 + (average[1] - etr[1])**2 + (average[2]-etr[2])**2 + (average[3] - etr[3])**2 + (average[4]-etr[4])**2 + (average[5] - etr[5])**2)**0.5\n",
        "    #     if(distance > parameter):\n",
        "    #       deleted.append(i)\n",
        "    #     i += 1\n",
        "    #   '''secondCompletion = time.time()\n",
        "    #   print(\"Second loop complete at: \" + str(secondCompletion))\n",
        "    #   print(str(secondCompletion - firstCompletion))\n",
        "    #   print(deleted)'''\n",
        "    #   return deleted\n",
        "      \n",
        "    #     #average = (etr[0][0] + 1)/2) + (etr[0][1] + 0.31)/1.92 + (etr[0][2] + 1.6)/3.3 + (etr[0][3] + 1.96)/2.41 + (etr[0][4] + 3.21)/6.05 + (etr[0][5] + 5.56)/10.84  \n",
        "\n",
        "    def inverse_threshold_prune(self, parameter):\n",
        "      i = 0;\n",
        "      deleted = []\n",
        "      deleted.clear()\n",
        "      while i < len(self._experience_replay_buffer): \n",
        "          etr = self._experience_replay_buffer._buffer[i]\n",
        "          if etr[2] < -parameter or etr[2] > parameter:\n",
        "              deleted.append(i)\n",
        "          i += 1\n",
        "      return deleted\n",
        "\n",
        "    def threshold_prune(self, parameter):\n",
        "      i = 0;\n",
        "      deleted = []\n",
        "      deleted.clear()\n",
        "      while i < len(self._experience_replay_buffer): \n",
        "          etr = self._experience_replay_buffer._buffer[i]\n",
        "          if etr[2] > -parameter and etr[2] < parameter:\n",
        "              deleted.append(i)\n",
        "          i += 1\n",
        "      return deleted\n",
        "\n",
        "    def cluster_prune(self, num_clusters, prune_percent):\n",
        "      deleted = []\n",
        "      # use proportional pruning based on previous replay buffer distributions (use equal distribution for first pruning)\n",
        "      # no sorting, only assign an entry to a cluster when it is checked, count how many are pruned from each cluster\n",
        "      '''\n",
        "      Reward Based Clustering:\n",
        "      '''\n",
        "      cluster_size = 2 / num_clusters\n",
        "      clusters = [-1 + j*cluster_size for j in range(num_clusters-1)]\n",
        "      cluster_list = [[] for _ in range(num_clusters + 1)]\n",
        "      i = 0\n",
        "      while i < len(self._experience_replay_buffer):\n",
        "        reward = self._experience_replay_buffer._buffer[i][2]\n",
        "        cluster_list[bisect.bisect_left(clusters, reward) + 1].append(i)\n",
        "        i+=1\n",
        "      for cluster in cluster_list:\n",
        "        num_to_remove = int(len(cluster) * prune_percent)\n",
        "        cluster[:] = random.sample(cluster, num_to_remove)\n",
        "      deleted = sum(cluster_list, [])\n",
        "      return deleted\n",
        "\n",
        "      #i=0\n",
        "      #while i < len(self._experience_replay_buffer):\n",
        "\n",
        "    def random_prune(self, p):\n",
        "      deleted = []\n",
        "      for i in range(len(self._experience_replay_buffer)):\n",
        "        if np.random.rand() < p:\n",
        "          deleted.append(i)\n",
        "      return deleted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wZ6C8TWibR6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rebQNkRdMkvC"
      },
      "outputs": [],
      "source": [
        "def linear_decay_schedule(episode_number: int,\n",
        "                          slope: float,\n",
        "                          minimum_epsilon: float) -> float:\n",
        "    \"\"\"Simple linear decay schedule used in the Deepmind paper.\"\"\"\n",
        "    return max(1 - slope * episode_number, minimum_epsilon)\n",
        "\n",
        "def power_decay_schedule(episode_number: int,\n",
        "                         decay_factor: float,\n",
        "                         minimum_epsilon: float) -> float:\n",
        "    \"\"\"Power decay schedule found in other practical applications.\"\"\"\n",
        "    return max(decay_factor**episode_number, minimum_epsilon)\n",
        "\n",
        "_epsilon_decay_schedule_kwargs = {\n",
        "    \"decay_factor\": 0.995,\n",
        "    \"minimum_epsilon\": 1e-2,\n",
        "}\n",
        "epsilon_decay_schedule = lambda n: power_decay_schedule(n, **_epsilon_decay_schedule_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h7mcR0gMnUR"
      },
      "outputs": [],
      "source": [
        "_optimizer_kwargs = {\n",
        "    \"lr\": 1e-2,\n",
        "    \"alpha\": 0.99,\n",
        "    \"eps\": 1e-08,\n",
        "    \"weight_decay\": 0,\n",
        "    \"momentum\": 0,\n",
        "    \"centered\": False\n",
        "}\n",
        "optimizer_fn = lambda parameters: optim.RMSprop(parameters, **_optimizer_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuvT0EofMpb6"
      },
      "outputs": [],
      "source": [
        "_agent_kwargs = {\n",
        "    \"state_size\": env.observation_space.shape[0],\n",
        "    \"action_size\": env.action_space.n, \n",
        "    \"number_hidden_units\": 64,\n",
        "    \"optimizer_fn\": optimizer_fn,\n",
        "    \"epsilon_decay_schedule\": epsilon_decay_schedule,\n",
        "    \"batch_size\": 64,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"alpha\": 1e-3,\n",
        "    \"gamma\": 0.99,\n",
        "    \"update_frequency\": 4,\n",
        "    \"seed\": None,\n",
        "}\n",
        "deep_q_agent = DeepQAgent(**_agent_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTh1rcxZMsBa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "def simulate(agent: Agent, env: gym.Env, ax: plt.Axes) -> None:\n",
        "    state = env.reset()\n",
        "    img = ax.imshow(env.render(mode='rgb_array'))\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        img.set_data(env.render(mode='rgb_array')) \n",
        "        plt.axis('off')\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        state, reward, done, _ = env.step(action)       \n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rOY7-eCRRHz0",
        "outputId": "fd1211fb-7736-4039-f536-4c91ed1fbe5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef prune(deep_q):\\n  \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "def prune(deep_q, parameter):\n",
        "  i = 0;\n",
        "  deleted = []\n",
        "  indexes = []\n",
        "  rewards = []\n",
        "  while i < len(deep_q._experience_replay_buffer):\n",
        "      etr = deep_q._experience_replay_buffer._buffer[i]\n",
        "      i += 1 \n",
        "      indexes.append(i)\n",
        "      rewards.append(etr[2])\n",
        "      if etr[2] > -parameter and etr[2] < parameter:\n",
        "          deleted.append(i)\n",
        "  plt.scatter(indexes, rewards)\n",
        "  plt.show()\n",
        "  return deleted\n",
        "\n",
        "print(prune(deep_q_agent, 0.1))\n",
        "'''\n",
        "\n",
        "'''\n",
        "def prune(deep_q):\n",
        "  \n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "       # Compare all entries to that average; if they are too far, they are outliers and can be removed UNLESS r > x\n",
        "      # Option Four: Threshold\n",
        "      # r > x OR r < y, keep it, otherwise delete [s, a, s1, r]\n",
        "      # Option One: Average\n",
        "      # Take average entry data (s, a, s1, r)\n",
        "      # Option Two: Sort\n",
        "      # Sort all entries from least to greatest (normalization?)\n",
        "      # Measure intervals between each entry: if the interval is too small, then they are too close to each other and one can be removed (the one farther away from the next two closest)\n",
        "      # Option Three: Probability\n",
        "      # Somehow determine a way to check probability of each state occurring (maybe if the coordinates are very far left/right, etc)\n",
        "      # Quantify this probability and remove the lowest probability ones (continuous -> discrete)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_vBlzMEMudD"
      },
      "outputs": [],
      "source": [
        "#_, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "#simulate(deep_q_agent, env, ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "GlGj5ZpLPcdI",
        "outputId": "3ba8dc00-9d88-4287-989f-82d2c6a25ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 25\tAverage Score: -182.71\n",
            "Episode 50\tAverage Score: -140.10\n",
            "Episode 75\tAverage Score: -151.25\n",
            "Episode 100\tAverage Score: -135.94\n",
            "Episode 125\tAverage Score: -121.05\n",
            "Episode 150\tAverage Score: -252.09\n",
            "Episode 175\tAverage Score: -167.85\n",
            "Episode 200\tAverage Score: -200.88\n",
            "Episode 225\tAverage Score: -163.21\n",
            "Episode 250\tAverage Score: -130.13\n",
            "Episode 275\tAverage Score: -146.13\n",
            "Episode 300\tAverage Score: -137.91\n",
            "Episode 325\tAverage Score: -129.28\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9b4900829583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_q_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoint.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-158fef847c31>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env, checkpoint_filepath, target_score, number_episodes, maximum_timesteps)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#start = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaximum_timesteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_train_until_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_train_for_at_most\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximum_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-158fef847c31>\u001b[0m in \u001b[0;36m_train_until_done\u001b[0;34m(agent, env)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-754ca67d64db>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;31m# every so often the agent should learn from experiences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_timesteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_sufficient_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                 \u001b[0mdeleted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter_prune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m                 \u001b[0;31m#print(\"Deleted: \" + str(len(deleted)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0;31m#print(\"Replay Buffer: \" + str(len(self._experience_replay_buffer)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-754ca67d64db>\u001b[0m in \u001b[0;36mparameter_prune\u001b[0;34m(self, parameter)\u001b[0m\n\u001b[1;32m    231\u001b[0m       \u001b[0mdeleted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m       \u001b[0mdeleted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m       \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experience_replay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m           \u001b[0metr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experience_replay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0metr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mparameter\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0metr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "scores = train(deep_q_agent, env, \"checkpoint.pth\", number_episodes=300, target_score=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8DSlVCsQZYZ"
      },
      "outputs": [],
      "source": [
        "#_, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "#simulate(deep_q_agent, env, ax)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}